load("C:/Users/lrowley/OneDrive/Documents/French Election Project/Milestone 2/Tweet Data/Combined Tweet dataset users 1 to 600.Rda")
View(combined_results)
seq_along(c(1,100))
?data.table::rbindlist
?makeCluster
split(d, ceiling(seq_along(1:100)/20))
split(x, ceiling(seq_along(x)/20))
x<- 1:100
split(x, ceiling(seq_along(x)/20))
seq_along(x)
seq_along(x)/20)
seq_along(x)/20
ceiling(seq_along(x)/20)
?ceiling()
library(jsonlite)
library(dplyr)
library(lubridate)
library(stringr)
library(parallel)
library(tidyr)
n_cores<- detectCores() - 2
library(jsonlite)
library(dplyr)
library(lubridate)
library(stringr)
library(parallel)
library(tidyr)
setwd("C:/Users/lrowley/OneDrive/Documents/French Election Project/Analysis Stage/Data preparation")
rm(list = ls()
setwd("C:/Users/lrowley/OneDrive/Documents/French Election Project/Analysis Stage/Data preparation")
rm(list = ls())
# Get path to raw JSON files on Univeristy of Nottingham OneDrive
dir<- "C:/Users/lrowley/The University of Nottingham/William Daniel (staff) - Twitter scrapes/elite_france_2022"
# Get list of JSON files. There is one for each Twitter account
files<- list.files(dir, recursive = T)
files<- paste0(dir, "/", files)
# Create list of variables to extract from JSON files
keep_cols<- c("created_at", "id_str", "full_text", "favorite_count", "retweet_count", "is_quote_status", "is_retweet", "user_id_str", "user_screen_name")
# Define function that can loop over a list of files to extract these variables. This function
# will be run in parallel across different cores to speed up processing of files.
read_tweet_json<- function(file_range){
# Load packages. Required as function will be running in different instances of R.
library(jsonlite)
library(dplyr)
library(lubridate)
library(stringr)
library(parallel)
library(tidyr)
# Nest within tryCatch so that if an error occurs on one core it returns NULL rather than stopping all cores
output_df<- tryCatch(
{ suppressWarnings(
# Loop over the file range passed to the function
for (i in file_range){
# Read in file
file<- files[i]
raw_string<- readLines(con = file)
combined_string<- paste0("[", paste0(raw_string, collapse = ","), "]")
df<- fromJSON(combined_string)
# Get nested key user info and add as normal variable
df$user_screen_name<- df$user$screen_name
df$user_id_str<- df$user$id_str
# Create variable indicating if Tweet is a Retweet
if(!"retweeted_status" %in% colnames(df)){
df$is_retweet<- FALSE
} else {
df$is_retweet<- lapply(df$retweeted_status[[1]], FUN = function(x){return(!is.na(x))})%>%
unlist()
}
# Keep the relevant variables
df<- df%>%
select(any_of(keep_cols))
# Add extracted data to a dataframe to output
if(exists("all_tweets")){
# Make sure variables match between new data and existing data
df<- select(df, any_of(colnames(all_tweets)))
# Make sure rownames are just rownumbers
rownames(df)<- c(nrow(all_tweets)+1):c(nrow(all_tweets)+nrow(df))
# Bind new data to existing data
all_tweets<- data.table::rbindlist(list(all_tweets, df))
} else {
all_tweets<- df
}
}
# Return output dataframe from tryCatch attempt
return(all_tweets)
x<- read_tweet_json(1:2)
# Define function that can loop over a list of files to extract these variables. This function
# will be run in parallel across different cores to speed up processing of files.
read_tweet_json<- function(file_range){
# Load packages. Required as function will be running in different instances of R.
library(jsonlite)
library(dplyr)
library(lubridate)
library(stringr)
library(parallel)
library(tidyr)
# Nest within tryCatch so that if an error occurs on one core it returns NULL rather than stopping all cores
output_df<- tryCatch(
{ suppressWarnings(
# Loop over the file range passed to the function
for (i in file_range){
# Read in file
file<- files[i]
raw_string<- readLines(con = file)
combined_string<- paste0("[", paste0(raw_string, collapse = ","), "]")
df<- fromJSON(combined_string)
# Get nested key user info and add as normal variable
df$user_screen_name<- df$user$screen_name
df$user_id_str<- df$user$id_str
# Create variable indicating if Tweet is a Retweet
if(!"retweeted_status" %in% colnames(df)){
df$is_retweet<- FALSE
} else {
df$is_retweet<- lapply(df$retweeted_status[[1]], FUN = function(x){return(!is.na(x))})%>%
unlist()
}
# Keep the relevant variables
df<- df%>%
select(any_of(keep_cols))
# Add extracted data to a dataframe to output
if(exists("all_tweets")){
# Make sure variables match between new data and existing data
df<- select(df, any_of(colnames(all_tweets)))
# Make sure rownames are just rownumbers
rownames(df)<- c(nrow(all_tweets)+1):c(nrow(all_tweets)+nrow(df))
# Bind new data to existing data
all_tweets<- data.table::rbindlist(list(all_tweets, df))
} else {
all_tweets<- df
}
}
# Return output dataframe from tryCatch attempt
return(all_tweets)
# Define function that can loop over a list of files to extract these variables. This function
# will be run in parallel across different cores to speed up processing of files.
read_tweet_json<- function(file_range){
# Load packages. Required as function will be running in different instances of R.
library(jsonlite)
library(dplyr)
library(lubridate)
library(stringr)
library(parallel)
library(tidyr)
# Nest within tryCatch so that if an error occurs on one core it returns NULL rather than stopping all cores
output_df<- tryCatch(
{ suppressWarnings(
# Loop over the file range passed to the function
for (i in file_range){
# Read in file
file<- files[i]
raw_string<- readLines(con = file)
combined_string<- paste0("[", paste0(raw_string, collapse = ","), "]")
df<- fromJSON(combined_string)
# Get nested key user info and add as normal variable
df$user_screen_name<- df$user$screen_name
df$user_id_str<- df$user$id_str
# Create variable indicating if Tweet is a Retweet
if(!"retweeted_status" %in% colnames(df)){
df$is_retweet<- FALSE
} else {
df$is_retweet<- lapply(df$retweeted_status[[1]], FUN = function(x){return(!is.na(x))})%>%
unlist()
}
# Keep the relevant variables
df<- df%>%
select(any_of(keep_cols))
# Add extracted data to a dataframe to output
if(exists("all_tweets")){
# Make sure variables match between new data and existing data
df<- select(df, any_of(colnames(all_tweets)))
# Make sure rownames are just rownumbers
rownames(df)<- c(nrow(all_tweets)+1):c(nrow(all_tweets)+nrow(df))
# Bind new data to existing data
all_tweets<- data.table::rbindlist(list(all_tweets, df))
} else {
all_tweets<- df
}
}
# Return output dataframe from tryCatch attempt
return(all_tweets)
# Define function that can loop over a list of files to extract these variables. This function
# will be run in parallel across different cores to speed up processing of files.
read_tweet_json<- function(file_range){
# Load packages. Required as function will be running in different instances of R.
library(jsonlite)
library(dplyr)
library(lubridate)
library(stringr)
library(parallel)
library(tidyr)
# Nest within tryCatch so that if an error occurs on one core it returns NULL rather than stopping all cores
output_df<- tryCatch(
{ suppressWarnings({
# Loop over the file range passed to the function
for (i in file_range){
# Read in file
file<- files[i]
raw_string<- readLines(con = file)
combined_string<- paste0("[", paste0(raw_string, collapse = ","), "]")
df<- fromJSON(combined_string)
# Get nested key user info and add as normal variable
df$user_screen_name<- df$user$screen_name
df$user_id_str<- df$user$id_str
# Create variable indicating if Tweet is a Retweet
if(!"retweeted_status" %in% colnames(df)){
df$is_retweet<- FALSE
} else {
df$is_retweet<- lapply(df$retweeted_status[[1]], FUN = function(x){return(!is.na(x))})%>%
unlist()
}
# Keep the relevant variables
df<- df%>%
select(any_of(keep_cols))
# Add extracted data to a dataframe to output
if(exists("all_tweets")){
# Make sure variables match between new data and existing data
df<- select(df, any_of(colnames(all_tweets)))
# Make sure rownames are just rownumbers
rownames(df)<- c(nrow(all_tweets)+1):c(nrow(all_tweets)+nrow(df))
# Bind new data to existing data
all_tweets<- data.table::rbindlist(list(all_tweets, df))
} else {
all_tweets<- df
}
}
# Return output dataframe from tryCatch attempt
return(all_tweets)
})
},
error = function(cond){
return(NULL)
}
)
# Return output dataframe from read_tweet_json function
return(output_df)
}
x<- read_tweet_json(1:2)
View(x)
x<- read_tweet_json(4000)
x<- read_tweet_json(c(1,4000))
# Define function that can loop over a list of files to extract these variables. This function
# will be run in parallel across different cores to speed up processing of files.
read_tweet_json<- function(file_range){
# Load packages. Required as function will be running in different instances of R.
library(jsonlite)
library(dplyr)
library(lubridate)
library(stringr)
library(parallel)
library(tidyr)
# Nest within tryCatch so that if an error occurs on one core it returns NULL rather than stopping all cores
output_df<- tryCatch(
{
# Ignore any warnings so that they don't trigger the tryCatch warning handler
suppressWarnings({
# Loop over the file range passed to the function
for (i in file_range){
# Read in file
file<- files[i]
raw_string<- readLines(con = file)
combined_string<- paste0("[", paste0(raw_string, collapse = ","), "]")
df<- fromJSON(combined_string)
# Get nested key user info and add as normal variable
df$user_screen_name<- df$user$screen_name
df$user_id_str<- df$user$id_str
# Create variable indicating if Tweet is a Retweet
if(!"retweeted_status" %in% colnames(df)){
df$is_retweet<- FALSE
} else {
df$is_retweet<- lapply(df$retweeted_status[[1]], FUN = function(x){return(!is.na(x))})%>%
unlist()
}
# Keep the relevant variables
df<- df%>%
select(any_of(keep_cols))
# Add extracted data to a dataframe to output
if(exists("all_tweets")){
# Make sure variables match between new data and existing data
df<- select(df, any_of(colnames(all_tweets)))
# Make sure rownames are just rownumbers
rownames(df)<- c(nrow(all_tweets)+1):c(nrow(all_tweets)+nrow(df))
# Bind new data to existing data
all_tweets<- data.table::rbindlist(list(all_tweets, df))
} else {
all_tweets<- df
}
}
# Return output dataframe from tryCatch attempt
return(all_tweets)
})
},
error = function(cond){
return(NULL)
}
)
# Return output dataframe from read_tweet_json function
return(output_df)
}
# Use two fewer cores than the available cores
n_cores<- detectCores() - 2
# Create a cluster of cores running R to run in parallel
cl<- makeCluster(n_cores)
file_ranges<- list(1:2, 4000)
# Export the relevant R objects to the cluster
clusterExport(cl = cl, varlist = c("read_tweet_json", "file_range", "files", "keep_cols"))
# Export the relevant R objects to the cluster
clusterExport(cl = cl, varlist = c("read_tweet_json", "file_ranges", "files", "keep_cols"))
# Apply the read_tweet_json function to each set of files, in parallel across the cluster of cores
results<- parLapply(cl, file_ranges, read_tweet_json)
View(results)
# Stop the cluster of cores
stopCluster(cl)
# Create a cluster of cores running R to run in parallel
cl<- makeCluster(n_cores)
# Split files indexes into a set of n lists where n is n_cores. Each core will read the files in its given range.
file_ranges<- split(1:length(files), ceiling(seq_along(1:length(files))/20))
file_ranges
class(file_ranges)
# Export the relevant R objects to the cluster
clusterExport(cl = cl, varlist = c("read_tweet_json", "file_ranges", "files", "keep_cols"))
# Apply the read_tweet_json function to each set of files, in parallel across the cluster of cores
results<- parLapply(cl, file_ranges, read_tweet_json)
